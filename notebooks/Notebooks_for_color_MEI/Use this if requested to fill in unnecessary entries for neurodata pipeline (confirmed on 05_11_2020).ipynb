{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(dic, filename):\n",
    "    \n",
    "    with h5py.File(filename, 'w') as h5file:\n",
    "        recursively_save_dict_contents_to_group(h5file, '/', dic)\n",
    "\n",
    "def load_dict_from_hdf5(filename):\n",
    "\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        return recursively_load_dict_contents_from_group(h5file, '/')\n",
    "\n",
    "\n",
    "\n",
    "def recursively_save_dict_contents_to_group( h5file, path, dic):\n",
    "\n",
    "    # argument type checking\n",
    "    if not isinstance(dic, dict):\n",
    "        raise ValueError(\"must provide a dictionary\")        \n",
    "\n",
    "    if not isinstance(path, str):\n",
    "        raise ValueError(\"path must be a string\")\n",
    "    if not isinstance(h5file, h5py._hl.files.File):\n",
    "        raise ValueError(\"must be an open h5py file\")\n",
    "    # save items to the hdf5 file\n",
    "    for key, item in dic.items():\n",
    "        #print(key,item)\n",
    "        key = str(key)\n",
    "        if isinstance(item, list):\n",
    "            item = np.array(item)\n",
    "            #print(item)\n",
    "        if not isinstance(key, str):\n",
    "            raise ValueError(\"dict keys must be strings to save to hdf5\")\n",
    "        # save strings, numpy.int64, and numpy.float64 types\n",
    "        if isinstance(item, (np.int64, np.float64, str, np.float, float, np.float32,int)):\n",
    "            #print( 'here' )\n",
    "            h5file[path + key] = item\n",
    "            if not h5file[path + key].value == item:\n",
    "                raise ValueError('The data representation in the HDF5 file does not match the original dict.')\n",
    "        # save numpy arrays\n",
    "        elif isinstance(item, np.ndarray):            \n",
    "            try:\n",
    "                h5file[path + key] = item\n",
    "            except:\n",
    "                item = np.array(item).astype('|S9')\n",
    "                h5file[path + key] = item\n",
    "            if not np.array_equal(h5file[path + key].value, item):\n",
    "                raise ValueError('The data representation in the HDF5 file does not match the original dict.')\n",
    "        # save dictionaries\n",
    "        elif isinstance(item, dict):\n",
    "            recursively_save_dict_contents_to_group(h5file, path + key + '/', item)\n",
    "        # other types cannot be saved and will result in an error\n",
    "        else:\n",
    "            #print(item)\n",
    "            raise ValueError('Cannot save %s type.' % type(item))\n",
    "\n",
    "def recursively_load_dict_contents_from_group( h5file, path): \n",
    "\n",
    "    ans = {}\n",
    "    for key, item in h5file[path].items():\n",
    "        if isinstance(item, h5py._hl.dataset.Dataset):\n",
    "            ans[key] = item.value\n",
    "        elif isinstance(item, h5py._hl.group.Group):\n",
    "            ans[key] = recursively_load_dict_contents_from_group(h5file, path + key + '/')\n",
    "    return ans            \n",
    "def recursively_make_fake_dict(dic, temp_dict={}):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    for key, item in dic.items():\n",
    "        if not isinstance(item, dict):\n",
    "            \n",
    "            try:\n",
    "                item_length = len(item)\n",
    "                if item_length > 10:\n",
    "                    temp_dict[key] = np.zeros(700)\n",
    "                else:\n",
    "                    temp_dict[key] = np.zeros(item_length)\n",
    "                \n",
    "            except:\n",
    "                # single value\n",
    "                temp_dict[key] = 0.0\n",
    "                \n",
    "        elif isinstance(item, dict):\n",
    "            temp_dict[key] = {}\n",
    "            recursively_make_fake_dict(item, temp_dict[key])\n",
    "        else:\n",
    "            raise ValueError('Cannot save %s type'%type(item))\n",
    "            \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "orig = load_dict_from_hdf5(\"/mnt/scratch07/synicix_dev/datasets/static22845-10-5-preproc0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['behavior', 'condition_hashes', 'images', 'item_info', 'neurons', 'pupil_center', 'responses', 'statistics', 'tiers', 'trial_idx', 'types'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting donnie@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "stimulus = dj.create_virtual_module('stimulus','pipeline_stimulus')\n",
    "imagenet = dj.create_virtual_module('imagenet','pipeline_imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cases = ['biased_correlated', 'biased_uncorrelated', 'unbiased_correlated', 'unbiased_uncorrelated']\n",
    "classes = ['imagenet_v2_rgb',\n",
    "           'imagenet_v2_rgb_g_b_no_normalization',\n",
    "           'imagenet_v2_rgb_range_30_225_mean127',\n",
    "           'imagenet_v2_rgb_g_b_channels_separately_joined']\n",
    "\n",
    "img_dict = dict()\n",
    "\n",
    "for case, _class in zip(image_cases, classes):\n",
    "    img_dict[case] = dict(img_class= _class, img_ids= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _key, _val in img_dict.items():\n",
    "    \n",
    "    if _key == 'unbiased_correlated' or _key == 'unbiased_uncorrelated':\n",
    "        # this works for now cuz there is only 1 album. later we might need more restriction\n",
    "        img_dict[_key]['img_ids'] = (stimulus.StaticImage.Image & 'image_class = \"{}\"'.format(img_dict[_key]['img_class'])).fetch('image_id')\n",
    "    \n",
    "    elif _key == 'biased_correlated':\n",
    "        img_dict[_key]['img_ids'] =(stimulus.StaticImage.Image & 'image_class = \"imagenet_v2_rgb_range_30_225_mean127\"').fetch('image_id')\n",
    "                \n",
    "\n",
    "    elif _key == 'biased_uncorrelated':\n",
    "        \n",
    "        img_dict[_key]['img_ids'] = (stimulus.StaticImage.Image & 'image_class = \"imagenet_v2_rgb_g_b_channels_separately_joined\"').fetch('image_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition_hashes\n",
    "condition_hashes = [str.encode('fake_condition_{}'.format(i)) for i in range(5000)]\n",
    "for oracle_inds in range(100):\n",
    "    condition_hashes +=[str.encode('fake_condition_{}'.format(oracle_inds+5000))] * 10\n",
    "\n",
    "condition_hashes = np.array(condition_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biased_correlated': {'img_class': 'imagenet_v2_rgb',\n",
       "  'img_ids': array([    62,    121,    151, ..., 124635, 124642, 124645])},\n",
       " 'biased_uncorrelated': {'img_class': 'imagenet_v2_rgb_g_b_no_normalization',\n",
       "  'img_ids': array([   0,    1,    2, ..., 5097, 5098, 5099])},\n",
       " 'unbiased_correlated': {'img_class': 'imagenet_v2_rgb_range_30_225_mean127',\n",
       "  'img_ids': array([    62,    121,    151, ..., 124635, 124642, 124645])},\n",
       " 'unbiased_uncorrelated': {'img_class': 'imagenet_v2_rgb_g_b_channels_separately_joined',\n",
       "  'img_ids': array([   0,    1,    2, ..., 5097, 5098, 5099])}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate item_info\n",
    "def generate_item_info(img_dict):\n",
    "    colorframeprojector_channel_1 = np.array([str.encode('2')]* 6000)\n",
    "    colorframeprojector_channel_2 = np.array([str.encode('3')]* 6000)\n",
    "    colorframeprojector_channel_3 = np.array([str.encode('None')]* 6000)\n",
    "    colorframeprojector_last_flip = np.arange(6000)\n",
    "    colorframeprojector_pre_blank_period = np.array([0.3] * 6000)\n",
    "    colorframeprojector_presentation_time = np.array([0.5] * 6000)\n",
    "    colorframeprojector_projector_config_id = np.ones(6000) * 6\n",
    "    colorframeprojector_projector_id = np.ones(6000, dtype=np.int64)\n",
    "    colorframeprojector_trial_ts = np.array([str.encode(\"Timestamp('2020-03-04 13:54:51')\")] * 6000)\n",
    "    condition_hash = condition_hashes\n",
    "    animal_id = np.zeros(6000,dtype=np.uint32)\n",
    "    scan_idx = np.zeros(6000, dtype=np.uint32)\n",
    "    session = np.zeros(6000, dtype=np.uint32)\n",
    "    trial_idx = np.arange(6000, dtype=np.uint32)\n",
    "\n",
    "    \n",
    "    for _key, _val in img_dict.items():\n",
    "        \n",
    "        colorframeprojector_image_class = np.array([str.encode(img_dict[_key]['img_class'])] * 6000)\n",
    "        colorframeprojector_image_id = img_dict[_key]['img_ids']\n",
    "\n",
    "\n",
    "        item_info = dict(animal_id = animal_id,\n",
    "                         colorframeprojector_channel_1 = colorframeprojector_channel_1,\n",
    "                         colorframeprojector_channel_2 = colorframeprojector_channel_2,\n",
    "                         colorframeprojector_channel_3 = colorframeprojector_channel_3,\n",
    "                         colorframeprojector_last_flip = colorframeprojector_last_flip,\n",
    "                         colorframeprojector_pre_blank_period = colorframeprojector_pre_blank_period,\n",
    "                         colorframeprojector_presentation_time = colorframeprojector_presentation_time,\n",
    "                         colorframeprojector_projector_config_id = colorframeprojector_projector_config_id,\n",
    "                         colorframeprojector_projector_id = colorframeprojector_projector_id,\n",
    "                         colorframeprojector_trial_ts = colorframeprojector_trial_ts,\n",
    "                         condition_hash = condition_hash,\n",
    "                         colorframeprojector_image_class = colorframeprojector_image_class,\n",
    "                         colorframeprojector_image_id = colorframeprojector_image_id,\n",
    "                         scan_idx = scan_idx,\n",
    "                         session = session,\n",
    "                         trial_idx = trial_idx\n",
    "                        )\n",
    "\n",
    "        img_dict[_key]['item_info'] = item_info\n",
    "    return img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict = generate_item_info(img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate statistics\n",
    "statistics = recursively_make_fake_dict(orig['statistics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate types\n",
    "types = np.array([str.encode('stimulus.ColorFrameProjector')]*6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate neurons\n",
    "unit_ids = np.arange(700, dtype=np.uint16)\n",
    "area = np.array([str.encode('V1')]*700)\n",
    "layer = np.array([str.encode('L2/3')]*700)\n",
    "trial_idx = np.arange(6000, dtype=np.uint32)\n",
    "\n",
    "neurons = dict(area = area,\n",
    "               layer = layer,\n",
    "               animal_id = np.zeros(6000,dtype=np.uint32),\n",
    "               scan_idx = np.zeros(6000, dtype=np.uint32),\n",
    "               session = np.zeros(6000, dtype=np.uint32),\n",
    "               trial_idx = trial_idx\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiers\n",
    "tiers = []\n",
    "for i in range(4950):\n",
    "    tiers.append(str.encode(\"train\"))\n",
    "for i in range(4950,5000):\n",
    "    tiers.append(str.encode(\"validation\"))\n",
    "for i in range(5000,6000):\n",
    "    tiers.append(str.encode(\"test\"))\n",
    "tiers = np.array(tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _key in img_dict.keys():\n",
    "    \n",
    "    \n",
    "    \n",
    "    with h5py.File(\"/mnt/scratch07/synicix_dev/datasets/\"+_key+\"_gabor_poisson.h5\",\"r\") as old_fh:\n",
    "        \n",
    "        \n",
    "        complete_dict = dict(images = old_fh['images'][:],\n",
    "                             true_rate = old_fh['true_rate'][:],\n",
    "                             responses = old_fh['responses'][:],\n",
    "                             behavior = old_fh['behavior'][:],\n",
    "                             pupil_center = old_fh['pupil_center'][:],\n",
    "                             tiers = tiers,\n",
    "                             item_info = img_dict[_key]['item_info'],\n",
    "                             statistics = statistics,\n",
    "                             neurons = neurons,\n",
    "                             trial_idx = trial_idx,\n",
    "                             types = types,\n",
    "                             condition_hashes = condition_hashes\n",
    "                            )\n",
    "        \n",
    "        \n",
    "        save_dict_to_hdf5(complete_dict,\"{}_gabor_poisson_new.h5\".format(_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biased_correlated\n",
      "---------------------------\n",
      "(6000, 2, 36, 64)\n",
      "(6000, 700)\n",
      "(6000, 700)\n",
      "(6000, 3)\n",
      "(6000, 3)\n",
      "hjhh\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "biased_uncorrelated\n",
      "---------------------------\n",
      "(6000, 2, 36, 64)\n",
      "(6000, 700)\n",
      "(6000, 700)\n",
      "(6000, 3)\n",
      "(6000, 3)\n",
      "hjhh\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "unbiased_correlated\n",
      "---------------------------\n",
      "(6000, 2, 36, 64)\n",
      "(6000, 700)\n",
      "(6000, 700)\n",
      "(6000, 3)\n",
      "(6000, 3)\n",
      "hjhh\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "unbiased_uncorrelated\n",
      "---------------------------\n",
      "(6000, 2, 36, 64)\n",
      "(6000, 700)\n",
      "(6000, 700)\n",
      "(6000, 3)\n",
      "(6000, 3)\n",
      "hjhh\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "for _key in img_dict.keys():\n",
    "     with h5py.File(_key+\"_gabor_poisson_new.h5\",\"r\") as old_fh:\n",
    "            \n",
    "            print(_key)\n",
    "            print(\"---------------------------\")\n",
    "            print(old_fh['images'][:].shape)\n",
    "            print(old_fh['true_rate'][:].shape)\n",
    "            print(old_fh['responses'][:].shape)\n",
    "            print(old_fh['behavior'][:].shape)\n",
    "            print(old_fh['pupil_center'][:].shape)\n",
    "            print('hjhh')\n",
    "            print(old_fh['tiers'][:].shape)\n",
    "            print(old_fh['trial_idx'][:].shape)\n",
    "            print(old_fh['types'].shape)\n",
    "            print(condition_hashes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tiers == 'test'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n",
      "(12000,)\n",
      "(12000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "for _key in img_dict.keys():\n",
    "    \n",
    "    \n",
    "    \n",
    "    with h5py.File(\"/mnt/scratch07/synicix_dev/datasets/\"+_key+\"_gabor_poisson.h5\",\"r\") as old_fh:\n",
    "        \n",
    "        print(old_fh['tiers'][:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'unbiased_uncorrelated_gabor_poisson_new.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-616f665dcc96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfh_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unbiased_uncorrelated_gabor_poisson_new.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'unbiased_uncorrelated_gabor_poisson_new.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "fh_new = h5py.File('unbiased_uncorrelated_gabor_poisson_new.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_new2 = h5py.File('biased_correlated_gabor_poisson_new.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
